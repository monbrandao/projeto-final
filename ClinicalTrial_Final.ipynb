{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClinicalTrial_Final.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monbrandao/projeto-final/blob/main/ClinicalTrial_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlSkVMek-KGm"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUQ3KYwHA37N"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "\n",
        "#nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8IR0fEbFqQz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "workdir_path = '/content/drive/My Drive/clinicaltrialsproject/records/'  # Inserir o local da pasta onde estão os arquivos de entrada (treino e teste)\n",
        "os.chdir(workdir_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9vJX-Gafm_G"
      },
      "source": [
        "# Importando arquivos XML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ijmNXoEFu2k"
      },
      "source": [
        "import pandas as pd \n",
        "from glob import glob\n",
        "import re\n",
        "\n",
        "import xml.etree.ElementTree as et \n",
        "\n",
        "file_names=glob(\"*.xml\")\n",
        "cont = 0\n",
        "\n",
        "dataset=[]\n",
        "for file_name in file_names: \n",
        "\n",
        "  xtree = et.parse(file_name)\n",
        "  \n",
        "  xroot = xtree.getroot()\n",
        "  text=xroot.find('TEXT').text.replace(\"\\n\",  '')\n",
        "  text=\"{0:>18}\".format(file_name) + ' - ' + re.sub(' +', ' ', text)\n",
        " # text= re.sub(' +', ' ', text)\n",
        "  dataset.append(text) \n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snWat2f2JzK9"
      },
      "source": [
        "record = pd.DataFrame(dataset, columns=['description'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCDFsKEdpXfv"
      },
      "source": [
        "def tp(descricao):\n",
        "   \n",
        "    if descricao.strip() == 'Record date':\n",
        "        val = 'PR'  \n",
        "    else:\n",
        "        val = 'CT'\n",
        "    return val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8WyInCcRxuv"
      },
      "source": [
        "criar novas colunas no dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc_EGxi9wUxf"
      },
      "source": [
        "  record['file_name'] = record['description'].str[:18]\n",
        "  \n",
        "  #record['record_date'] = record['description'].str[:44].str[-10:]\n",
        "\n",
        "  record['doc_type'] =  record['description'].str[:32].str[-12:].apply(tp)\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfV9XLPSyVin"
      },
      "source": [
        "# corrigr campo data\n",
        "record.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoXfR3wV4wZx"
      },
      "source": [
        "record.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QciQc9Tufvwi"
      },
      "source": [
        "# Pré - Processamento de texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOtzDWgkYsIm"
      },
      "source": [
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "newline = lambda x: re.sub('\\n', ' ', x) # remove \\n\n",
        "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x) # remove alphanumeric words\n",
        "simbols = lambda x: re.sub('\\W', ' ', x) # remove alphanumeric words\n",
        "under = lambda x: re.sub('_', ' ', x) # remove hypen words\n",
        "punc_re = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x) # remove puntuação\n",
        "#inclui para tirar palavaras\n",
        "lower_alpha = lambda x: re.sub(r\"\"\"\\w*\\d\\w*\"\"\", ' ', x.lower())\n",
        "\n",
        "\n",
        "#record['description']  = record.description.map(newline).map(lower_alpha).map(punc_re).map(under)\n",
        "record['description']  = record.description.map(newline).map(under).map(simbols).map(alphanumeric).map(punc_re).map(lower_alpha)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQXDNRgvZa_L"
      },
      "source": [
        "record['description']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-9Uww6UWvB3"
      },
      "source": [
        "# Tokenização"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KiKmSiGV9Um"
      },
      "source": [
        "record['tokens'] = record.description.map(word_tokenize)\n",
        "record.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NdtryGNtTuU"
      },
      "source": [
        "word_list = sum(record.tokens.tolist(), [])\n",
        "\n",
        "word_list[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJLq_wv5W67T"
      },
      "source": [
        "# Stopwords "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T37ZwR2fYII2"
      },
      "source": [
        "# Estas são as etapas para encontrar as palavras mais comuns\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Converte a lista em um dicionário com contagem de valores\n",
        "word_counts = Counter(word_list)\n",
        "\n",
        "# Inverter a chave / valores no dicionário para classificarReverter a chave / valores no dicionário para classificar\n",
        "word_counts = list(zip(word_counts.values(), word_counts.keys()))\n",
        "\n",
        "# Classifique a lista por contagem\n",
        "word_counts = sorted(word_counts, reverse=True)\n",
        "\n",
        "# Imprime as 10 palavras mais comuns\n",
        "word_counts[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clA6H_vN4iGj"
      },
      "source": [
        "def removeStops(texts, stopwords):\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        texts_out.append(\" \".join([token for token in sent if token not in stopwords]))\n",
        "    return texts_out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-BJuHindSwX"
      },
      "source": [
        "# Remova as stop words\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "stop_words += [\"mg\", \"po\", \"qd\", \"dr\", \"p\", \"l\", \"pt\", \"xml\", \"ga\", \"date\", \"jr\", \"har\",\"h\",\"patient\", \"history\", \"pain\", \"tablet\"]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo_KOVMYhIse"
      },
      "source": [
        "# ... more stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WYu8-fjgrue"
      },
      "source": [
        "stop_lambda = lambda x: [y for y in x if y not in stop_words]\n",
        "record['tokens_stop'] = record.tokens.apply(stop_lambda)\n",
        "record.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CsQMFVhdyHI"
      },
      "source": [
        "word_list = sum(record.tokens_stop.tolist(), [])\n",
        "\n",
        "word_list[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_cI_7oDfM_v"
      },
      "source": [
        "# partes de speech tagging\n",
        "from nltk.tag import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos_lambda = lambda x: nltk.pos_tag(x)\n",
        "record['tokens_pos'] = (record.tokens_stop.apply(pos_lambda))\n",
        "record.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPDTb6hge6F7"
      },
      "source": [
        "\n",
        "word_list_clean = sum(record.tokens_stop.tolist(), [])\n",
        "\n",
        "word_list_clean[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvlTjcF9gWCE"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Converte a lista em um dicionário com contagem de valores\n",
        "word_counts_clean = Counter(word_list_clean)\n",
        "a = word_counts_clean\n",
        "\n",
        "# Inverta a chave / valores no dicionário para classificar\n",
        "word_counts_clean = list(zip(word_counts_clean.values(), word_counts_clean.keys()))\n",
        "\n",
        "# Classifique a lista por contagem\n",
        "word_counts_clean = sorted(word_counts_clean, reverse=True)\n",
        "\n",
        "# Imprime as 10 palavras mais comuns\n",
        "word_counts_clean[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRIaSp4bgkcj"
      },
      "source": [
        "# Worldcloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTZHDh2NTLVQ"
      },
      "source": [
        "from wordcloud import WordCloud,STOPWORDS\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckpGO2JdR_zX"
      },
      "source": [
        "plt.figure(figsize = (10,10)) # Text that is not Fake\n",
        "wc = WordCloud(background_color='white',max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).fit_words(a)\n",
        "plt.imshow(wc , interpolation = 'bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d6DButXe6mG"
      },
      "source": [
        "# Gerar o Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hBpRSFhc5JZ"
      },
      "source": [
        "corpus = record.description\n",
        "corpus.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaaEHyT5tgyo"
      },
      "source": [
        "# Criada uma matriz de termos de documento usando o Count Vectorizer com as stopwords ativadas para inglês\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "          \n",
        "cv = CountVectorizer(stop_words=\"english\",\n",
        "                            analyzer='word',      \n",
        "                             lowercase=True,)\n",
        "\n",
        "term_frequencies = cv.fit_transform(corpus).toarray()\n",
        "vocab = cv.get_feature_names ()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ggmc9qSXpInq"
      },
      "source": [
        "dt = pd.DataFrame(term_frequencies, columns=cv.get_feature_names()).set_index(record.file_name) \n",
        "dt.head(9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLarw2G8wEqX"
      },
      "source": [
        "# Encontre o Registro mais parecido com o Clinical Trial NCT03986073  usando o TF-IDF Vectorizer (comparar documentos)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr6fCfKZwJRK"
      },
      "source": [
        "record[record.file_name=='CT_NCT03986073.xml'].description"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-4ZTl-twXr0"
      },
      "source": [
        "clinicaltrial = list(dt.loc['CT_NCT03986073.xml'])\n",
        "clinicaltrial[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DJXPmGlwqHT"
      },
      "source": [
        "# Definir o cálculo de similaridade de cosseno\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "cosine = lambda v1, v2: dot(v1, v2) / (norm(v1) * norm(v2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQp0Pc_Nsjlf"
      },
      "source": [
        "#Similiaridade do Cosseno"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtQUjeMMwtSg"
      },
      "source": [
        "similarity = [cosine(clinicaltrial, file_name) for file_name in term_frequencies]\n",
        "sorted(list(zip(similarity, record.file_name)), reverse=True)[1:9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayBRdseRsq41"
      },
      "source": [
        "#TF_IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uujTDBiXxSfm"
      },
      "source": [
        "# Crie o vetorizador TF-IDF - stopwords pq faço uma comparação matematica e por isso preciso tirar as palavras que não quero\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "          \n",
        "cv_tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
        "\n",
        "dt_tfidf = pd.DataFrame(X_tfidf, columns=cv_tfidf.get_feature_names()).set_index(record.file_name)\n",
        "dt_tfidf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mgI6_bcxeWo"
      },
      "source": [
        "# Calcule todas as semelhanças e classifique as mais semelhantes usando os dados do TF-IDF\n",
        "similarity_tfidf = [cosine(clinicaltrial, file_name) for file_name in X_tfidf]\n",
        "sorted(list(zip(similarity_tfidf, record.file_name)), reverse=True)[1:9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtA9O8kxLOSo"
      },
      "source": [
        "#LDA\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2UOOkq9EDkD"
      },
      "source": [
        "# cada letra de \n",
        "patient_record = [lyrics for lyrics in record.description] \n",
        "print(\"Temos %d registros de pacientes.\" %len(patient_record))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBeBqRIHLxf7"
      },
      "source": [
        "!pip install pyLDAvis\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjRPeL2EMC8f"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation\n",
        "from gensim.models import ldamodel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.sklearn\n",
        "\n",
        "\n",
        "# remove stopwords and punctuations\n",
        "def preprocess(row):\n",
        "    return strip_punctuation(remove_stopwords(row.lower()))\n",
        "    \n",
        "record['description'] = record['description'] .apply(preprocess)\n",
        "\n",
        "# Convert data to required input format by LDA\n",
        "texts = []\n",
        "for line in record.description:\n",
        "    lowered = line.lower()\n",
        "    words = re.findall(r'\\w+', lowered, flags = re.UNICODE)# | re.LOCALE\n",
        "    texts.append(words)\n",
        "    \n",
        "# Create a dictionary representation of the documents.\n",
        "dictionary = Dictionary(texts)\n",
        "\n",
        "# Filter out words that occur less than 2 documents, or more than 30% of the documents.\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.3)\n",
        "\n",
        "# Bag-of-words representation of the documents.\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bib1Apjn8Mzk"
      },
      "source": [
        "## 2. Treinamos o modelo LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "IFFVKsGM8Mzl"
      },
      "source": [
        "# Set training parameters.\n",
        "num_topics = 10\n",
        "chunksize = 2000\n",
        "passes = 50\n",
        "iterations = 200\n",
        "eval_every = None\n",
        "\n",
        "# Train model\n",
        "model = ldamodel.LdaModel(corpus=corpus, id2word=dictionary, chunksize=chunksize, alpha='auto', \n",
        "                          eta='auto', iterations=iterations, num_topics=num_topics, passes=passes, \n",
        "                          eval_every=eval_every)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74WbtqNW8Mzn"
      },
      "source": [
        "## 3. Distribuição de topicos de documentos\n",
        "Usamos o método get_document_topics que realiza a infrencia de a distribuição de topicos de um documento. Retorna uma lista de tupa (topic_id, probability)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoSfSrsr8Mzo"
      },
      "source": [
        "# Get document topics\n",
        "all_topics = model.get_document_topics(corpus, minimum_probability=0)\n",
        "all_topics[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLkMfXHN8Mzr"
      },
      "source": [
        "## 4. Preparando os arquivos de entrada para TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6FKihY-8Mzs"
      },
      "source": [
        "from tensorboard_helper import CreateTensorboardData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "oKJ1WOAQ8Mzu"
      },
      "source": [
        "vectors = [ [topics[1] for topics in doc_topics] for doc_topics in all_topics]\n",
        "metadatos = [record.file_name.values, record.doc_type.values ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B9b5mo58Mzw"
      },
      "source": [
        "CreateTensorboardData(tensor_filename=\"doc_lda\", \n",
        "                      vectors=vectors, \n",
        "                      metadatos=metadatos,\n",
        "                      colnames=[\"file_name\",\"doc_type\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA_bL14z4Khi"
      },
      "source": [
        "**Visualizando usando PCA ...**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkYWb1b_3t-Z"
      },
      "source": [
        "Agora, vamos anexar os tópicos com maior probabilidade (topic_id, topic_probability) ao título do documento, para explorar quais tópicos os domínios ou bordas do cluster pertencem de forma dominante. Para isso, precisamos sobrescrever o arquivo de metadados conforme abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkGiFrnr4AXt"
      },
      "source": [
        "tensors = []\n",
        "for doc_topics in all_topics:\n",
        "    doc_tensor = []\n",
        "    for topic in doc_topics:\n",
        "        if round(topic[1], 3) > 0:\n",
        "            doc_tensor.append((topic[0], float(round(topic[1], 3))))\n",
        "            \n",
        "    # sort topics according to highest probabilities\n",
        "    doc_tensor = sorted(doc_tensor, key=lambda x: x[1], reverse=True)\n",
        "    # store vectors to add in metadata file\n",
        "    tensors.append(doc_tensor[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GUWw8hP4G8q"
      },
      "source": [
        "tensors_str = [ str(\" \".join([title, str(x)])) for title, x in zip(record.file_name.values, tensors)]\n",
        "metadatos = [tensors_str, record.doc_type.values]\n",
        "\n",
        "CreateTensorboardData(tensor_filename=\"doc_lda\", \n",
        "                      vectors=vectors, \n",
        "                      metadatos=metadatos,\n",
        "                      colnames=[\"file_name\",\"doc_type\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P9KyI7e3vLc"
      },
      "source": [
        "model.show_topic(topicid=0, topn=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_Pq3UmRPJAi"
      },
      "source": [
        "import pyLDAvis.gensim\n",
        "\n",
        "viz = pyLDAvis.gensim.prepare(model, corpus, dictionary)\n",
        "pyLDAvis.display(viz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObDVqUUq65uf"
      },
      "source": [
        "UsandoGridSearch para encontrar melhor modelo LDA\n",
        "O parâmetro de ajuste mais importante para modelos LDA é n_components (número de tópicos). Além disso, vamos pesquisar learning_decay (que controla a taxa de aprendizado)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6PYwEyGAYw8"
      },
      "source": [
        "vectorizer = CountVectorizer(analyzer='word',  \n",
        "                             lowercase=True,                   # convert all words to lowercase\n",
        "                             token_pattern='[a-zA-Z0-9]{5,}',  # num chars > 5\n",
        "                            )\n",
        "\n",
        "# data_vectorized = vectorizer.fit_transform(data_lemmatized)\n",
        "data_vectorized = vectorizer.fit_transform(record['description'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejsk-o2u65AO"
      },
      "source": [
        "# Define Search Param\n",
        "search_params = {'n_components': [5, 10, 15], 'learning_decay': [.5, .7, .9]}\n",
        "\n",
        "# Init the Model\n",
        "lda = LatentDirichletAllocation()\n",
        "\n",
        "# Init Grid Search Class\n",
        "model = GridSearchCV(lda, param_grid=search_params)\n",
        "\n",
        "# Do the Grid Search\n",
        "model.fit(data_vectorized)\n",
        "\n",
        "#(model, corpus, dictionary)\n",
        "#(lda_model, data_vectorized, vectorizer) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivpejd6UBhj1"
      },
      "source": [
        "Escolhendo o melhor modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbWBeMB9BhBZ"
      },
      "source": [
        "# Melhor modelo\n",
        "best_lda_model = model.best_estimator_\n",
        "\n",
        "# Hiperparâmetros do modelo\n",
        "print(\"Melhores parâmetros: \", model.best_params_)\n",
        "\n",
        "# probabilidade logarítmica\n",
        "print(\"Melhor score de probabilidade logarítmica: \", model.best_score_)\n",
        "\n",
        "# Perplexidade\n",
        "print(\"Perplexidade do modelo: \", best_lda_model.perplexity(data_vectorized))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czeeBNDhB3sp"
      },
      "source": [
        "#comparando o score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjp1z1aqDLm3"
      },
      "source": [
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dWiFtbnB51s"
      },
      "source": [
        "results = pd.DataFrame(model.cv_results_)\n",
        "\n",
        "current_palette = sns.color_palette(\"Set2\", 3)\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "sns.lineplot(data=results,\n",
        "             x='param_n_components',\n",
        "             y='mean_test_score',\n",
        "             hue='param_learning_decay',\n",
        "             palette=current_palette,\n",
        "             marker='o'\n",
        "            )\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chJZlWRmCIUj"
      },
      "source": [
        "# Create Document - Topic Matrix\n",
        "lda_output = best_lda_model.transform(data_vectorized)\n",
        "\n",
        "# column names\n",
        "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n",
        "\n",
        "# index names\n",
        "docnames = [\"Doc\" + str(i) for i in range(len(patient_record))]\n",
        "\n",
        "# Make the pandas dataframe\n",
        "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
        "\n",
        "# Get dominant topic for each document\n",
        "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
        "df_document_topic['dominant_topic'] = dominant_topic\n",
        "\n",
        "# Styling\n",
        "def color_green(val):\n",
        "    color = 'green' if val > .1 else 'black'\n",
        "    return 'color: {col}'.format(col=color)\n",
        "\n",
        "def make_bold(val):\n",
        "    weight = 700 if val > .1 else 400\n",
        "    return 'font-weight: {weight}'.format(weight=weight)\n",
        "\n",
        "# Apply Style\n",
        "df_document_topics = df_document_topic.style.applymap(color_green).applymap(make_bold)\n",
        "df_document_topics_first10 = df_document_topic[:10].style.applymap(color_green).applymap(make_bold)\n",
        "df_document_topics_first10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIyfS4_pCPBQ"
      },
      "source": [
        "#Quantidade de documento por tópico"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGPVamt_CM4J"
      },
      "source": [
        "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
        "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
        "df_topic_distribution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMRgy4UYCe1O"
      },
      "source": [
        "#Visualizando LDA com pyLDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhGXJllYCbtM"
      },
      "source": [
        "pyLDAvis.enable_notebook() \n",
        "panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne') \n",
        "pyLDAvis.display(panel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q12IfFPmGJuc"
      },
      "source": [
        "# Principais palavras por tópico"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaqE1KF3F5Ia"
      },
      "source": [
        "vocab = vectorizer.get_feature_names()\n",
        "\n",
        "# data_vectorized\n",
        "topic_words = {}\n",
        "n_top_words = 20\n",
        "\n",
        "for topic, comp in enumerate(best_lda_model.components_):\n",
        "    # for the n-dimensional array \"arr\":\n",
        "    # argsort() returns a ranked n-dimensional array of arr, call it \"ranked_array\"\n",
        "    # which contains the indices that would sort arr in a descending fashion\n",
        "    # for the ith element in ranked_array, ranked_array[i] represents the index of the\n",
        "    # element in arr that should be at the ith index in ranked_array\n",
        "    # ex. arr = [3,7,1,0,3,6]\n",
        "    # np.argsort(arr) -> [3, 2, 0, 4, 5, 1]\n",
        "    # word_idx contains the indices in \"topic\" of the top num_top_words most relevant\n",
        "    # to a given topic ... it is sorted ascending to begin with and then reversed (desc. now)    \n",
        "    word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
        "\n",
        "    # store the words most relevant to the topic\n",
        "    topic_words[topic] = [vocab[i] for i in word_idx]\n",
        "    \n",
        "for topic, words in topic_words.items():\n",
        "    print('Topic: %d' % topic)\n",
        "    print('  %s' % ', '.join(words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPOdJg0BGHNP"
      },
      "source": [
        "# tranformando objeto style em um dataframe pandas\n",
        "df2 = pd.DataFrame(data=df_document_topics.data, columns=df_document_topics.columns)\n",
        "df2.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kjKeSgEGXcr"
      },
      "source": [
        "df2[\"file_name\"] = record[\"file_name\"].tolist()\n",
        "df2.tail(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2ZTmqPzHzOm"
      },
      "source": [
        "df2[df2[\"dominant_topic\"]==2].groupby([\"file_name\"]).size().sort_values(ascending=False)[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKQNvYrQGdqg"
      },
      "source": [
        "df2[df2[\"dominant_topic\"]==2].sort_values(by=['Topic2'], ascending=False)[:15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2Lx2nbVY-6B"
      },
      "source": [
        "\n",
        "\n",
        "# Print the top 10 words per topic\n",
        "n_words = 10\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "topic_list = []\n",
        "for topic_idx, topic in enumerate(best_lda_model.components_):\n",
        "    top_n = [feature_names[i]\n",
        "             for i in topic.argsort()\n",
        "             [-n_words:]][::-1]\n",
        "    top_features = ' '.join(top_n)\n",
        "    topic_list.append(f\"topic_{'_'.join(top_n[:3])}\") \n",
        "\n",
        "    print(f\"Topic {topic_idx}: {top_features}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfBDcyumWchm"
      },
      "source": [
        "amounts = model.transform(data_vectorized) * 100\n",
        "\n",
        "# Set it up as a dataframe\n",
        "topics = pd.DataFrame(amounts, columns=topic_list)\n",
        "topics.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpKhPrXDziMa"
      },
      "source": [
        "#Doc2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb7uM_Sm0nxn"
      },
      "source": [
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYiTSj9tzUZx"
      },
      "source": [
        "def read_corpus(documents):\n",
        "    for i, plot in enumerate(documents):\n",
        "        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(plot, max_len=30), [i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5nMRtu2zrs0"
      },
      "source": [
        "train_corpus = list(read_corpus(record.description))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL1UtmCQ00r5"
      },
      "source": [
        "## 2. Treinando o modelo doc2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f45--6-O02xC"
      },
      "source": [
        "model = gensim.models.doc2vec.Doc2Vec(size=50, min_count=2, iter=55)\n",
        "model.build_vocab(train_corpus)\n",
        "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puOyTBSY31yp"
      },
      "source": [
        "#RANKEAR OS DOCUMENTOS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVkpzC4fvYLS"
      },
      "source": [
        "ranks = []\n",
        "second_ranks = []\n",
        "for doc_id in range(len(train_corpus)):\n",
        "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
        "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
        "    rank = [docid for docid, sim in sims].index(doc_id)\n",
        "    ranks.append(rank)\n",
        "\n",
        "    second_ranks.append(sims[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRPTG3ijtavf"
      },
      "source": [
        "import collections\n",
        "\n",
        "counter = collections.Counter(ranks)\n",
        "print(counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INeHYXwYv3uY"
      },
      "source": [
        "print( 'Clinical Trial - Document number {} : file_name - {} - «{}»\\n'.format(doc_id,  record.file_name[doc_id] ,' '.join(train_corpus[doc_id].words)))\n",
        "print(u'Similiarity of the documents per model using Word2ve %s:\\n' % model)\n",
        "for label, index in [('MOST SIMILAR', 1), ('SECOND-MOST SIMILAR', 2), ('THIRD-MOST SIMILAR', 3), ('MEDIAN', len(sims)//2), ('LEAST SIMILAR', len(sims) - 1)]:  \n",
        "  print(u'* %s %s : file_name -%s-: «%s»\\n' % (label, sims[index],  record.file_name[sims[index][0]].strip(), ' '.join(train_corpus[sims[index][0]].words)))\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ-i6tGX1L3w"
      },
      "source": [
        "tensor_file = 'doc_tensor.w2v'\n",
        "model.save_word2vec_format(tensor_file, doctag_vec=True, word_vec=False)\n",
        "key_vectors = gensim.models.KeyedVectors.load_word2vec_format(tensor_file, binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8HLCuq91ZWt"
      },
      "source": [
        "descriptions_id = []\n",
        "vectors = []\n",
        "for description_id in key_vectors.index2word:\n",
        "    descriptions_id.append(description_id)\n",
        "    vectors.append(key_vectors[description_id])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYGgISHyDNdU"
      },
      "source": [
        "from tensorboard_helper import CreateTensorboardData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN1Aj65b1eA0"
      },
      "source": [
        "3. Preparando os arquivos de entrada para TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBdcRik21fBh"
      },
      "source": [
        "metadatos=[descriptions_id]\n",
        "CreateTensorboardData(tensor_filename=\"patient_record\", \n",
        "                      vectors=vectors, \n",
        "                      metadatos=[descriptions_id])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wckSyr6C1pTS"
      },
      "source": [
        "metadatos=[record.file_name.values, record.doc_type.values]\n",
        "\n",
        "CreateTensorboardData(tensor_filename=\"patient_record\", \n",
        "                      vectors=vectors, \n",
        "                      metadatos=metadatos,\n",
        "                      colnames=[\"file_name\",\"doc_type\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}